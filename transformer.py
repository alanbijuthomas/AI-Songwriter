# -*- coding: utf-8 -*-
"""transformer.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1tQQzjNC3Gt3_Ho0wT_X-esyxwRlZbhCP
"""

!nvidia-smi -L

!pip install -q aitextgen

from aitextgen import aitextgen
from nltk.translate.bleu_score import corpus_bleu
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.metrics.pairwise import cosine_similarity
import numpy as np

# use small GPT-2 model for fine-tuning
model = aitextgen(tf_gpt2="124M", to_gpu=True)

# train model on dataset
model.train('dataset1.txt',
            line_by_line=False,
            from_cache=False,
            num_steps=1000,
            generate_every=1000,
            save_every=1000,
            save_gdrive=False,
            learning_rate=1e-3,
            fp16=False,
            batch_size=1,
            )

# reload model from storage
model = aitextgen(model='transformer.bin', config='config.json', to_gpu=True)

# generate lyrics
model.generate(max_length=512, top_k=50, top_p=0.9, repetition_penalty=1.5)

# read dataset from text file
with open('dataset1.txt', 'r') as file:
  dataset_str = file.read()

# split dataset into lines
dataset_lst = dataset_str.split('\n')

# get rid of empty lines in dataset
dataset_lst = [line for line in dataset_lst if line.strip() != '']

# get rid of duplicate lines in dataset
dataset_lst = list(dict.fromkeys(dataset_lst))

# store BLEU scores
bleu_scores = []

# run trial 100 times
for i in range(100):
  # candidate sentences are generated lyrics
  lyrics = model.generate_one(top_k=50, top_p=0.9, repetition_penalty=1.5)
  candidates = lyrics.split('\n')

  # split candidate sentences into words
  for i in range(len(candidates)):
    line = candidates[i].split()
    candidates[i] = line

  # reference sentences are dataset lyrics
  references = []
  for i in range(len(candidates)):
    references.append(dataset_lst)

  # calculate BLEU score
  bleu_score = corpus_bleu(references, candidates)

  # append to BLEU scores
  bleu_scores.append(bleu_score)

# print average BLEU score
print('BLEU: ', np.average(bleu_scores))

# store similarity scores
similarity_scores = []

# run trial 100 times
for i in range(100):
  # generate lyrics to compare to dataset
  lyrics = model.generate_one(top_k=50, top_p=0.9, repetition_penalty=1.5)
  lyrics = lyrics.split('\n')

  # for dataset, learn tf-idf and transform to document matrix
  tfidf_dataset = TfidfVectorizer().fit_transform(dataset_lst)

  # learn tf-idf for dataset
  tfidf_lyrics = TfidfVectorizer().fit(dataset_lst)
  # transform to document matrix for generated lyrics
  tfidf_lyrics = tfidf_lyrics.transform(lyrics)

  # calculate cosine similarities between dataset, generated lyrics
  similarities = cosine_similarity(tfidf_dataset, tfidf_lyrics).flatten()

  # find max similarity
  max_similarity = np.amax(similarities)

  # append to similarity scores
  similarity_scores.append(max_similarity)

# print average similarity score
print('Similarity: ', np.average(similarity_scores))
